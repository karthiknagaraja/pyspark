{"cells":[{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\ndataFrame = spark.createDataFrame(zip([1,2,3],[5,6,7],[7,8,9]),['A','B','C'])\ndisplay(dataFrame)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>A</th><th>B</th><th>C</th></tr></thead><tbody><tr><td>1</td><td>5</td><td>7</td></tr><tr><td>2</td><td>6</td><td>8</td></tr><tr><td>3</td><td>7</td><td>9</td></tr></tbody></table></div>"]}}],"execution_count":1},{"cell_type":"code","source":["ve = VectorAssembler(inputCols=['A','B','C'], outputCol='vec_feature')\nout = ve.transform(dataFrame)\ndisplay(out)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>A</th><th>B</th><th>C</th><th>vec_feature</th></tr></thead><tbody><tr><td>1</td><td>5</td><td>7</td><td>List(1, 3, List(), List(1.0, 5.0, 7.0))</td></tr><tr><td>2</td><td>6</td><td>8</td><td>List(1, 3, List(), List(2.0, 6.0, 8.0))</td></tr><tr><td>3</td><td>7</td><td>9</td><td>List(1, 3, List(), List(3.0, 7.0, 9.0))</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.ml.feature import StandardScaler, MinMaxScaler\n\nmm_scaler = MinMaxScaler(inputCol='vec_feature', outputCol='minmax_scaled')\nss_scaler = StandardScaler(inputCol='vec_feature', outputCol='standard_scaled', withMean=True, withStd=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["mm = mm_scaler.fit(out)\nss = ss_scaler.fit(out)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["help(StopWordsRemover)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on class StopWordsRemover in module pyspark.ml.feature:\n\nclass StopWordsRemover(pyspark.ml.wrapper.JavaTransformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n  A feature transformer that filters out stop words from input.\n  \n  .. note:: null values from input array are preserved unless adding null to stopWords explicitly.\n  \n  &gt;&gt;&gt; df = spark.createDataFrame([([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],)], [&quot;text&quot;])\n  &gt;&gt;&gt; remover = StopWordsRemover(inputCol=&quot;text&quot;, outputCol=&quot;words&quot;, stopWords=[&quot;b&quot;])\n  &gt;&gt;&gt; remover.transform(df).head().words == [&apos;a&apos;, &apos;c&apos;]\n  True\n  &gt;&gt;&gt; stopWordsRemoverPath = temp_path + &quot;/stopwords-remover&quot;\n  &gt;&gt;&gt; remover.save(stopWordsRemoverPath)\n  &gt;&gt;&gt; loadedRemover = StopWordsRemover.load(stopWordsRemoverPath)\n  &gt;&gt;&gt; loadedRemover.getStopWords() == remover.getStopWords()\n  True\n  &gt;&gt;&gt; loadedRemover.getCaseSensitive() == remover.getCaseSensitive()\n  True\n  \n  .. versionadded:: 1.6.0\n  \n  Method resolution order:\n      StopWordsRemover\n      pyspark.ml.wrapper.JavaTransformer\n      pyspark.ml.wrapper.JavaParams\n      pyspark.ml.wrapper.JavaWrapper\n      pyspark.ml.base.Transformer\n      pyspark.ml.param.shared.HasInputCol\n      pyspark.ml.param.shared.HasOutputCol\n      pyspark.ml.param.Params\n      pyspark.ml.util.Identifiable\n      pyspark.ml.util.JavaMLReadable\n      pyspark.ml.util.MLReadable\n      pyspark.ml.util.JavaMLWritable\n      pyspark.ml.util.MLWritable\n      builtins.object\n  \n  Methods defined here:\n  \n  __init__(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=False, locale=None)\n      __init__(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=false,         locale=None)\n  \n  getCaseSensitive(self)\n      Gets the value of :py:attr:&#96;caseSensitive&#96; or its default value.\n      \n      .. versionadded:: 1.6.0\n  \n  getLocale(self)\n      Gets the value of :py:attr:&#96;locale&#96;.\n      \n      .. versionadded:: 2.4.0\n  \n  getStopWords(self)\n      Gets the value of :py:attr:&#96;stopWords&#96; or its default value.\n      \n      .. versionadded:: 1.6.0\n  \n  setCaseSensitive(self, value)\n      Sets the value of :py:attr:&#96;caseSensitive&#96;.\n      \n      .. versionadded:: 1.6.0\n  \n  setLocale(self, value)\n      Sets the value of :py:attr:&#96;locale&#96;.\n      \n      .. versionadded:: 2.4.0\n  \n  setParams(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=False, locale=None)\n      setParams(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=false,         locale=None)\n      Sets params for this StopWordRemover.\n      \n      .. versionadded:: 1.6.0\n  \n  setStopWords(self, value)\n      Sets the value of :py:attr:&#96;stopWords&#96;.\n      \n      .. versionadded:: 1.6.0\n  \n  ----------------------------------------------------------------------\n  Static methods defined here:\n  \n  loadDefaultStopWords(language)\n      Loads the default stop words for the given language.\n      Supported languages: danish, dutch, english, finnish, french, german, hungarian,\n      italian, norwegian, portuguese, russian, spanish, swedish, turkish\n      \n      .. versionadded:: 2.0.0\n  \n  ----------------------------------------------------------------------\n  Data and other attributes defined here:\n  \n  caseSensitive = Param(parent=&apos;undefined&apos;, name=&apos;caseSensitive&apos;, ...a c...\n  \n  locale = Param(parent=&apos;undefined&apos;, name=&apos;locale&apos;, doc=&apos;lo... the input...\n  \n  stopWords = Param(parent=&apos;undefined&apos;, name=&apos;stopWords&apos;, doc=&apos;The words...\n  \n  ----------------------------------------------------------------------\n  Data and other attributes inherited from pyspark.ml.wrapper.JavaTransformer:\n  \n  __metaclass__ = &lt;class &apos;abc.ABCMeta&apos;&gt;\n      Metaclass for defining Abstract Base Classes (ABCs).\n      \n      Use this metaclass to create an ABC.  An ABC can be subclassed\n      directly, and then acts as a mix-in class.  You can also register\n      unrelated concrete classes (even built-in classes) and unrelated\n      ABCs as &apos;virtual subclasses&apos; -- these and their descendants will\n      be considered subclasses of the registering ABC by the built-in\n      issubclass() function, but the registering ABC won&apos;t show up in\n      their MRO (Method Resolution Order) nor will method\n      implementations defined by the registering ABC be callable (not\n      even via super()).\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.wrapper.JavaParams:\n  \n  copy(self, extra=None)\n      Creates a copy of this instance with the same uid and some\n      extra params. This implementation first calls Params.copy and\n      then make a copy of the companion Java pipeline component with\n      extra params. So both the Python wrapper and the Java pipeline\n      component get copied.\n      \n      :param extra: Extra parameters to copy to the new instance\n      :return: Copy of this instance\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n  \n  __del__(self)\n  \n  ----------------------------------------------------------------------\n  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n  \n  __dict__\n      dictionary for instance variables (if defined)\n  \n  __weakref__\n      list of weak references to the object (if defined)\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.base.Transformer:\n  \n  transform(self, dataset, params=None)\n      Transforms the input dataset with optional parameters.\n      \n      :param dataset: input dataset, which is an instance of :py:class:&#96;pyspark.sql.DataFrame&#96;\n      :param params: an optional param map that overrides embedded params.\n      :returns: transformed dataset\n      \n      .. versionadded:: 1.3.0\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.param.shared.HasInputCol:\n  \n  getInputCol(self)\n      Gets the value of inputCol or its default value.\n  \n  setInputCol(self, value)\n      Sets the value of :py:attr:&#96;inputCol&#96;.\n  \n  ----------------------------------------------------------------------\n  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCol:\n  \n  inputCol = Param(parent=&apos;undefined&apos;, name=&apos;inputCol&apos;, doc=&apos;input colum...\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n  \n  getOutputCol(self)\n      Gets the value of outputCol or its default value.\n  \n  setOutputCol(self, value)\n      Sets the value of :py:attr:&#96;outputCol&#96;.\n  \n  ----------------------------------------------------------------------\n  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n  \n  outputCol = Param(parent=&apos;undefined&apos;, name=&apos;outputCol&apos;, doc=&apos;output co...\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.param.Params:\n  \n  explainParam(self, param)\n      Explains a single param and returns its name, doc, and optional\n      default value and user-supplied value in a string.\n  \n  explainParams(self)\n      Returns the documentation of all params with their optionally\n      default values and user-supplied values.\n  \n  extractParamMap(self, extra=None)\n      Extracts the embedded default param values and user-supplied\n      values, and then merges them with extra values from input into\n      a flat param map, where the latter value is used if there exist\n      conflicts, i.e., with ordering: default param values &lt;\n      user-supplied values &lt; extra.\n      \n      :param extra: extra param values\n      :return: merged param map\n  \n  getOrDefault(self, param)\n      Gets the value of a param in the user-supplied param map or its\n      default value. Raises an error if neither is set.\n  \n  getParam(self, paramName)\n      Gets a param by its name.\n  \n  hasDefault(self, param)\n      Checks whether a param has a default value.\n  \n  hasParam(self, paramName)\n      Tests whether this instance contains a param with a given\n      (string) name.\n  \n  isDefined(self, param)\n      Checks whether a param is explicitly set by user or has\n      a default value.\n  \n  isSet(self, param)\n      Checks whether a param is explicitly set by user.\n  \n  set(self, param, value)\n      Sets a parameter in the embedded param map.\n  \n  ----------------------------------------------------------------------\n  Data descriptors inherited from pyspark.ml.param.Params:\n  \n  params\n      Returns all params ordered by name. The default implementation\n      uses :py:func:&#96;dir&#96; to get all attributes of type\n      :py:class:&#96;Param&#96;.\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.util.Identifiable:\n  \n  __repr__(self)\n      Return repr(self).\n  \n  ----------------------------------------------------------------------\n  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n  \n  read() from builtins.type\n      Returns an MLReader instance for this class.\n  \n  ----------------------------------------------------------------------\n  Class methods inherited from pyspark.ml.util.MLReadable:\n  \n  load(path) from builtins.type\n      Reads an ML instance from the input path, a shortcut of &#96;read().load(path)&#96;.\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.util.JavaMLWritable:\n  \n  write(self)\n      Returns an MLWriter instance for this ML instance.\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.util.MLWritable:\n  \n  save(self, path)\n      Save this ML instance to the given path, a shortcut of &apos;write().save(path)&apos;.\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml.feature import FeatureHasher\n\ndataset = spark.createDataFrame([\n    (2.2, True, \"1\", \"foo\"),\n    (3.3, False, \"2\", \"bar\"),\n    (4.4, False, \"3\", \"baz\"),\n    (5.5, False, \"4\", \"foo\")\n], [\"real\", \"bool\", \"stringNum\", \"string\"])\n\nhasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n                       outputCol=\"features\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["featurized = hasher.transform(dataset)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["featurized.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+---------+------+--------------------------------------------------------+\nreal|bool |stringNum|string|features                                                |\n+----+-----+---------+------+--------------------------------------------------------+\n2.2 |true |1        |foo   |(262144,[174475,247670,257907,262126],[2.2,1.0,1.0,1.0])|\n3.3 |false|2        |bar   |(262144,[70644,89673,173866,174475],[1.0,1.0,1.0,3.3])  |\n4.4 |false|3        |baz   |(262144,[22406,70644,174475,187923],[1.0,1.0,4.4,1.0])  |\n5.5 |false|4        |foo   |(262144,[70644,101499,174475,257907],[1.0,1.0,5.5,1.0]) |\n+----+-----+---------+------+--------------------------------------------------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["help(FeatureHasher)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on class FeatureHasher in module pyspark.ml.feature:\n\nclass FeatureHasher(pyspark.ml.wrapper.JavaTransformer, pyspark.ml.param.shared.HasInputCols, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.param.shared.HasNumFeatures, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n  .. note:: Experimental\n  \n  Feature hashing projects a set of categorical or numerical features into a feature vector of\n  specified dimension (typically substantially smaller than that of the original feature\n  space). This is done using the hashing trick (https://en.wikipedia.org/wiki/Feature_hashing)\n  to map features to indices in the feature vector.\n  \n  The FeatureHasher transformer operates on multiple columns. Each column may contain either\n  numeric or categorical features. Behavior and handling of column data types is as follows:\n  \n  * Numeric columns:\n      For numeric features, the hash value of the column name is used to map the\n      feature value to its index in the feature vector. By default, numeric features\n      are not treated as categorical (even when they are integers). To treat them\n      as categorical, specify the relevant columns in &#96;categoricalCols&#96;.\n  \n  * String columns:\n      For categorical features, the hash value of the string &quot;column_name=value&quot;\n      is used to map to the vector index, with an indicator value of &#96;1.0&#96;.\n      Thus, categorical features are &quot;one-hot&quot; encoded\n      (similarly to using :py:class:&#96;OneHotEncoder&#96; with &#96;dropLast=false&#96;).\n  \n  * Boolean columns:\n      Boolean values are treated in the same way as string columns. That is,\n      boolean features are represented as &quot;column_name=true&quot; or &quot;column_name=false&quot;,\n      with an indicator value of &#96;1.0&#96;.\n  \n  Null (missing) values are ignored (implicitly zero in the resulting feature vector).\n  \n  Since a simple modulo is used to transform the hash function to a vector index,\n  it is advisable to use a power of two as the &#96;numFeatures&#96; parameter;\n  otherwise the features will not be mapped evenly to the vector indices.\n  \n  &gt;&gt;&gt; data = [(2.0, True, &quot;1&quot;, &quot;foo&quot;), (3.0, False, &quot;2&quot;, &quot;bar&quot;)]\n  &gt;&gt;&gt; cols = [&quot;real&quot;, &quot;bool&quot;, &quot;stringNum&quot;, &quot;string&quot;]\n  &gt;&gt;&gt; df = spark.createDataFrame(data, cols)\n  &gt;&gt;&gt; hasher = FeatureHasher(inputCols=cols, outputCol=&quot;features&quot;)\n  &gt;&gt;&gt; hasher.transform(df).head().features\n  SparseVector(262144, {174475: 2.0, 247670: 1.0, 257907: 1.0, 262126: 1.0})\n  &gt;&gt;&gt; hasher.setCategoricalCols([&quot;real&quot;]).transform(df).head().features\n  SparseVector(262144, {171257: 1.0, 247670: 1.0, 257907: 1.0, 262126: 1.0})\n  &gt;&gt;&gt; hasherPath = temp_path + &quot;/hasher&quot;\n  &gt;&gt;&gt; hasher.save(hasherPath)\n  &gt;&gt;&gt; loadedHasher = FeatureHasher.load(hasherPath)\n  &gt;&gt;&gt; loadedHasher.getNumFeatures() == hasher.getNumFeatures()\n  True\n  &gt;&gt;&gt; loadedHasher.transform(df).head().features == hasher.transform(df).head().features\n  True\n  \n  .. versionadded:: 2.3.0\n  \n  Method resolution order:\n      FeatureHasher\n      pyspark.ml.wrapper.JavaTransformer\n      pyspark.ml.wrapper.JavaParams\n      pyspark.ml.wrapper.JavaWrapper\n      pyspark.ml.base.Transformer\n      pyspark.ml.param.shared.HasInputCols\n      pyspark.ml.param.shared.HasOutputCol\n      pyspark.ml.param.shared.HasNumFeatures\n      pyspark.ml.param.Params\n      pyspark.ml.util.Identifiable\n      pyspark.ml.util.JavaMLReadable\n      pyspark.ml.util.MLReadable\n      pyspark.ml.util.JavaMLWritable\n      pyspark.ml.util.MLWritable\n      builtins.object\n  \n  Methods defined here:\n  \n  __init__(self, numFeatures=262144, inputCols=None, outputCol=None, categoricalCols=None)\n      __init__(self, numFeatures=1 &lt;&lt; 18, inputCols=None, outputCol=None, categoricalCols=None)\n  \n  getCategoricalCols(self)\n      Gets the value of binary or its default value.\n      \n      .. versionadded:: 2.3.0\n  \n  setCategoricalCols(self, value)\n      Sets the value of :py:attr:&#96;categoricalCols&#96;.\n      \n      .. versionadded:: 2.3.0\n  \n  setParams(self, numFeatures=262144, inputCols=None, outputCol=None, categoricalCols=None)\n      setParams(self, numFeatures=1 &lt;&lt; 18, inputCols=None, outputCol=None, categoricalCols=None)\n      Sets params for this FeatureHasher.\n      \n      .. versionadded:: 2.3.0\n  \n  ----------------------------------------------------------------------\n  Data and other attributes defined here:\n  \n  categoricalCols = Param(parent=&apos;undefined&apos;, name=&apos;categoricalCols&apos;, do...\n  \n  ----------------------------------------------------------------------\n  Data and other attributes inherited from pyspark.ml.wrapper.JavaTransformer:\n  \n  __metaclass__ = &lt;class &apos;abc.ABCMeta&apos;&gt;\n      Metaclass for defining Abstract Base Classes (ABCs).\n      \n      Use this metaclass to create an ABC.  An ABC can be subclassed\n      directly, and then acts as a mix-in class.  You can also register\n      unrelated concrete classes (even built-in classes) and unrelated\n      ABCs as &apos;virtual subclasses&apos; -- these and their descendants will\n      be considered subclasses of the registering ABC by the built-in\n      issubclass() function, but the registering ABC won&apos;t show up in\n      their MRO (Method Resolution Order) nor will method\n      implementations defined by the registering ABC be callable (not\n      even via super()).\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.wrapper.JavaParams:\n  \n  copy(self, extra=None)\n      Creates a copy of this instance with the same uid and some\n      extra params. This implementation first calls Params.copy and\n      then make a copy of the companion Java pipeline component with\n      extra params. So both the Python wrapper and the Java pipeline\n      component get copied.\n      \n      :param extra: Extra parameters to copy to the new instance\n      :return: Copy of this instance\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n  \n  __del__(self)\n  \n  ----------------------------------------------------------------------\n  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n  \n  __dict__\n      dictionary for instance variables (if defined)\n  \n  __weakref__\n      list of weak references to the object (if defined)\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.base.Transformer:\n  \n  transform(self, dataset, params=None)\n      Transforms the input dataset with optional parameters.\n      \n      :param dataset: input dataset, which is an instance of :py:class:&#96;pyspark.sql.DataFrame&#96;\n      :param params: an optional param map that overrides embedded params.\n      :returns: transformed dataset\n      \n      .. versionadded:: 1.3.0\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.param.shared.HasInputCols:\n  \n  getInputCols(self)\n      Gets the value of inputCols or its default value.\n  \n  setInputCols(self, value)\n      Sets the value of :py:attr:&#96;inputCols&#96;.\n  \n  ----------------------------------------------------------------------\n  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCols:\n  \n  inputCols = Param(parent=&apos;undefined&apos;, name=&apos;inputCols&apos;, doc=&apos;input col...\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n  \n  getOutputCol(self)\n      Gets the value of outputCol or its default value.\n  \n  setOutputCol(self, value)\n      Sets the value of :py:attr:&#96;outputCol&#96;.\n  \n  ----------------------------------------------------------------------\n  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n  \n  outputCol = Param(parent=&apos;undefined&apos;, name=&apos;outputCol&apos;, doc=&apos;output co...\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.param.shared.HasNumFeatures:\n  \n  getNumFeatures(self)\n      Gets the value of numFeatures or its default value.\n  \n  setNumFeatures(self, value)\n      Sets the value of :py:attr:&#96;numFeatures&#96;.\n  \n  ----------------------------------------------------------------------\n  Data and other attributes inherited from pyspark.ml.param.shared.HasNumFeatures:\n  \n  numFeatures = Param(parent=&apos;undefined&apos;, name=&apos;numFeatures&apos;, doc=&apos;numbe...\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.param.Params:\n  \n  explainParam(self, param)\n      Explains a single param and returns its name, doc, and optional\n      default value and user-supplied value in a string.\n  \n  explainParams(self)\n      Returns the documentation of all params with their optionally\n      default values and user-supplied values.\n  \n  extractParamMap(self, extra=None)\n      Extracts the embedded default param values and user-supplied\n      values, and then merges them with extra values from input into\n      a flat param map, where the latter value is used if there exist\n      conflicts, i.e., with ordering: default param values &lt;\n      user-supplied values &lt; extra.\n      \n      :param extra: extra param values\n      :return: merged param map\n  \n  getOrDefault(self, param)\n      Gets the value of a param in the user-supplied param map or its\n      default value. Raises an error if neither is set.\n  \n  getParam(self, paramName)\n      Gets a param by its name.\n  \n  hasDefault(self, param)\n      Checks whether a param has a default value.\n  \n  hasParam(self, paramName)\n      Tests whether this instance contains a param with a given\n      (string) name.\n  \n  isDefined(self, param)\n      Checks whether a param is explicitly set by user or has\n      a default value.\n  \n  isSet(self, param)\n      Checks whether a param is explicitly set by user.\n  \n  set(self, param, value)\n      Sets a parameter in the embedded param map.\n  \n  ----------------------------------------------------------------------\n  Data descriptors inherited from pyspark.ml.param.Params:\n  \n  params\n      Returns all params ordered by name. The default implementation\n      uses :py:func:&#96;dir&#96; to get all attributes of type\n      :py:class:&#96;Param&#96;.\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.util.Identifiable:\n  \n  __repr__(self)\n      Return repr(self).\n  \n  ----------------------------------------------------------------------\n  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n  \n  read() from builtins.type\n      Returns an MLReader instance for this class.\n  \n  ----------------------------------------------------------------------\n  Class methods inherited from pyspark.ml.util.MLReadable:\n  \n  load(path) from builtins.type\n      Reads an ML instance from the input path, a shortcut of &#96;read().load(path)&#96;.\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.util.JavaMLWritable:\n  \n  write(self)\n      Returns an MLWriter instance for this ML instance.\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.util.MLWritable:\n  \n  save(self, path)\n      Save this ML instance to the given path, a shortcut of &apos;write().save(path)&apos;.\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\ndf = spark.createDataFrame([\n    (Vectors.dense([2.0, 1.0]),),\n    (Vectors.dense([0.0, 0.0]),),\n    (Vectors.dense([3.0, -1.0]),)\n], [\"features\"])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["d = df.toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["d.to_csv('a.csv')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["df = spark.createDataFrame([(\"foo bar\",),(\"hello world\",)]).toDF(\"sentence\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\n   sentence|\n+-----------+\n    foo bar|\nhello world|\n+-----------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["from pyspark import keyword_only\nfrom pyspark.ml import Transformer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n \nclass StringLengthTransformer(Transformer, HasInputCol, HasOutputCol):\n \n   @keyword_only\n   def __init__(self, inputCol=None, outputCol=None):\n       super(StringLengthTransformer, self).__init__()\n       kwargs = self._input_kwargs\n       self.setParams(**kwargs)\n \n   @keyword_only\n   def setParams(self, inputCol=None, outputCol=None):\n       kwargs = self._input_kwargs\n       return self._set(**kwargs)\n \n   def _transform(self, dataset):\n       reverse = udf(lambda sentence: len(sentence), IntegerType())\n       return dataset.withColumn(self.getOutputCol(), reverse(dataset[self.getInputCol()]))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["strlength = StringLengthTransformer(inputCol=\"sentence\", outputCol=\"len\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&apos;inputCol&apos;: &apos;sentence&apos;, &apos;outputCol&apos;: &apos;len&apos;}\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["reverse = udf(lambda sentence: len(sentence), IntegerType())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["df = spark.createDataFrame([(\"foo bar\",),(\"hello world\",)]).toDF(\"sentence\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\n   sentence|\n+-----------+\n    foo bar|\nhello world|\n+-----------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["df.withColumn('len', reverse(df.sentence)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+\n   sentence|len|\n+-----------+---+\n    foo bar|  7|\nhello world| 11|\n+-----------+---+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["from pyspark import keyword_only\nfrom pyspark.ml import Transformer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n \nclass StringLengthTransformer(Transformer, HasInputCol, HasOutputCol):\n \n   @keyword_only\n   def __init__(self, inputCol=None, outputCol=None):\n       super(StringLengthTransformer, self).__init__()\n       kwargs = self._input_kwargs\n       self.setParams(**kwargs)\n \n   @keyword_only\n   def setParams(self, inputCol=None, outputCol=None):\n       kwargs = self._input_kwargs\n       return self._set(**kwargs)\n \n   def _transform(self, dataset):\n       def cv(s):\n          return  s.count('a') + s.count('e') + s.count('i') + s.count('o') + s.count('u')\n       count_vowel = udf(cv, IntegerType())\n       return dataset.withColumn(self.getOutputCol(), count_vowel(dataset[self.getInputCol()]))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["df = spark.createDataFrame([(\"foo bar\",),(\"hello world\",)]).toDF(\"sentence\")\nstrlength = StringLengthTransformer(inputCol=\"sentence\", outputCol=\"len\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["strlength.transform(df).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+\n   sentence|len|\n+-----------+---+\n    foo bar|  3|\nhello world|  3|\n+-----------+---+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":25}],"metadata":{"name":"Preprocessing","notebookId":2965463592442818},"nbformat":4,"nbformat_minor":0}
